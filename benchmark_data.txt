| Benchmark                                                     | Best-in-class score (model)                                         | Other strong models (≈ 2025-Q2 public releases)                                                                                                                                 | Notes                                                                                                                                      |
| ------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| **MMLU** (Massive Multitask Language Understanding, 5-shot)   | **91.8 %** — OpenAI **o1 (high-temp “thinking”)** ([openai.com][1]) | GPT-4.1 90.2 % ([openai.com][1]); Gemini Ultra v1 90.0 % ([blog.google][2]); GPT-4o 85.7 % ([openai.com][1])                                                                    | Widely considered *saturated*: many labs now treat ≥ 90 % as a solved target and rely on harder, private variants.                         |
| **GPQA Diamond** (graduate-level physics/chem/biology)        | **86.4 %** — Google **Gemini 2.5 Pro** ([vellum.ai][3])             | Grok 3 84.6 % ([vellum.ai][3]); OpenAI o3 83.3 % ([vellum.ai][3]); o4-mini 81.4 % ([vellum.ai][3]); GPT-4.1 66.3 % (uses OpenAI’s stricter extraction grader) ([openai.com][1]) | Still a differentiator: biology items remain the hardest slice; tool-augmented runs can push > 90 %.                                       |
| **MATH 500** (Vals-AI 500-question slice of Hendrycks et al.) | **95.2 %** — Gemini 2.5 Pro Exp ★ ([vals.ai][4])                    | Grok 3 Mini Fast 94.2 % ([vals.ai][4]); DeepSeek-R1 92.2 % ([vals.ai][4]); OpenAI o3-mini 91.8 % ([vals.ai][4]); GPT-4.1 mini 88.0 % ([vals.ai][4])                             | Scores > 90 % suggest heavy pre-training exposure; many labs are pivoting to private “FrontierMath” or AIME-2025 sets for cleaner signals. |

[1]: https://openai.com/index/gpt-4-1/ "Introducing GPT-4.1 in the API | OpenAI"
[2]: https://blog.google/technology/ai/google-gemini-ai/?utm_source=chatgpt.com "Introducing Gemini: our largest and most capable AI model"
[3]: https://www.vellum.ai/llm-leaderboard "LLM Leaderboard 2025"
[4]: https://www.vals.ai/benchmarks/math500-04-15-2025 "MATH 500"

